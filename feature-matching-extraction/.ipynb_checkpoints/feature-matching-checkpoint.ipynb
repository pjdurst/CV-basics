{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force matches\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for Jupyter Notebook display\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('source-sign.jpg', cv2.IMREAD_GRAYSCALE)  # Query Image\n",
    "img2 = cv2.imread('target-sign.jpg', cv2.IMREAD_GRAYSCALE)  # Train Image\n",
    "\n",
    "# Feature detector (SIFT)\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect and compute descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# Brute-Force Matcher (BFMatcher)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform Brute-Force matching\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute time taken\n",
    "bf_time = end_time - start_time\n",
    "print(f\"Brute-Force matching took {bf_time:.6f} seconds\")\n",
    "\n",
    "# Apply Lowe’s Ratio Test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display using Matplotlib (for Jupyter Notebook)\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img_matches, cmap='viridis')\n",
    "plt.title(\"Brute-Force Feature Matching\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN matches\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time  # Import time module\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('source-sign.jpg', 0)  # Query Image\n",
    "img2 = cv2.imread('target-sign.jpg', 0)  # Train Image\n",
    "\n",
    "# Feature detector (SIFT)\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect and compute descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# FLANN Parameters\n",
    "index_params = dict(algorithm=1, trees=5)  # KD-Tree index\n",
    "search_params = dict(checks=50)  # Number of search iterations\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform FLANN-based matching\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute time taken\n",
    "flann_time = end_time - start_time\n",
    "print(f\"FLANN-based matching took {flann_time:.6f} seconds\")\n",
    "\n",
    "# Apply Lowe’s Ratio Test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None)\n",
    "\n",
    "# Display the matches\n",
    "# Display using Matplotlib (for Jupyter Notebook)\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img_matches, cmap='viridis')\n",
    "plt.title(\"FLANN Feature Matching\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685ba26",
   "metadata": {},
   "source": [
    "Why did brute force run faster than FLANN? Because we only needed to find one object in a simple image. Let's try something more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force matches for a more complicated scene\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for Jupyter Notebook display\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('source-sign.jpg', cv2.IMREAD_GRAYSCALE)  # Query Image\n",
    "img2 = cv2.imread('target-sign-noisy.jpg', cv2.IMREAD_GRAYSCALE)  # Train Image\n",
    "\n",
    "# Feature detector (SIFT)\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect and compute descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# Brute-Force Matcher (BFMatcher)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform Brute-Force matching\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute time taken\n",
    "bf_time = end_time - start_time\n",
    "print(f\"Brute-Force matching took {bf_time:.6f} seconds\")\n",
    "\n",
    "# Apply Lowe’s Ratio Test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display using Matplotlib (for Jupyter Notebook)\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img_matches, cmap='viridis')\n",
    "plt.title(\"Brute-Force Feature Matching\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN matches\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time  # Import time module\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('source-sign.jpg', 0)  # Query Image\n",
    "img2 = cv2.imread('target-sign-noisy.jpg', 0)  # Train Image\n",
    "\n",
    "# Feature detector (SIFT)\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect and compute descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# FLANN Parameters\n",
    "index_params = dict(algorithm=1, trees=5)  # KD-Tree index\n",
    "search_params = dict(checks=50)  # Number of search iterations\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform FLANN-based matching\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute time taken\n",
    "flann_time = end_time - start_time\n",
    "print(f\"FLANN-based matching took {flann_time:.6f} seconds\")\n",
    "\n",
    "# Apply Lowe’s Ratio Test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None)\n",
    "\n",
    "# Display the matches\n",
    "# Display using Matplotlib (for Jupyter Notebook)\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img_matches, cmap='viridis')\n",
    "plt.title(\"FLANN Feature Matching\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2add8",
   "metadata": {},
   "source": [
    "So, none of that worked particularly well. Let's break down the pieces of FLANN and see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28343aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a source image and a target image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for Jupyter Notebook display\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('spongebob-target.jpg', cv2.IMREAD_GRAYSCALE)  # Query Image\n",
    "img2 = cv2.imread('bubble-source.jpg', cv2.IMREAD_GRAYSCALE)  # Train Image\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Target image\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img2)\n",
    "plt.title(\"Source feature\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "# Let's use SIFT detector to find the keypoints\n",
    "# This time we will adjust our own parameters for tuning with smart choices. The parameters are:\n",
    "# sift = cv2.SIFT_create(\n",
    "#    nfeatures=500, # the size of our feature vector, higher value = more keypoints returned\n",
    "#    contrastThreshold=0.03, # the acceptable difference (color, intensity, etc.) between a feature and its background, a lower value = more keypoints\n",
    "#    edgeThreshold=15, # the minimum number of pixels required to form an edge\n",
    "#    sigma=1.2 # the amount of Gaussian blurring done before edge and feature detection\n",
    "#)\n",
    "\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=500, # We can leave this low, since we can guess that there are only about 10 bubbles in the target image\n",
    "    contrastThreshold=0.1, # We can raise this, since the bubbles stand out from the background already\n",
    "    edgeThreshold=15, # We aren't dealing with many background features, so we don't have to mess with this\n",
    "    sigma=1.0 # We can lower the blur to help reduce false detections, again because the bubbles stand our clearly\n",
    ")\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# Brute-Force matcher with L2 norm distance\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "# Sort matches by distance (best matches first)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Draw the top N matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None, flags=2)\n",
    "\n",
    "# Display using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(img_matches)\n",
    "plt.title('SIFT Feature Matching')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d028dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've gotten pretty accurage feature detection, though we've clearly missed a few bubbles.\n",
    "# Now that we have our keypoints, we can apply FLANN\n",
    "\n",
    "# load in a source image and a target image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for Jupyter Notebook display\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('spongebob-target.jpg', cv2.IMREAD_GRAYSCALE)  # Query Image\n",
    "img2 = cv2.imread('bubble-source.jpg', cv2.IMREAD_GRAYSCALE)  # Train Image\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Target image\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.imshow(img2)\n",
    "plt.title(\"Source feature\")\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "# Let's use SIFT detector to find the keypoints\n",
    "# This time we will adjust our own parameters for tuning with smart choices. The parameters are:\n",
    "# sift = cv2.SIFT_create(\n",
    "#    nfeatures=500, # the size of our feature vector, higher value = more keypoints returned\n",
    "#    contrastThreshold=0.03, # the acceptable difference (color, intensity, etc.) between a feature and its background, a lower value = more keypoints\n",
    "#    edgeThreshold=15, # the minimum number of pixels required to form an edge\n",
    "#    sigma=1.2 # the amount of Gaussian blurring done before edge and feature detection\n",
    "#)\n",
    "\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=500, # We can leave this low, since we can guess that there are only about 10 bubbles in the target image\n",
    "    contrastThreshold=0.1, # We can raise this, since the bubbles stand out from the background already\n",
    "    edgeThreshold=15, # We aren't dealing with many background features, so we don't have to mess with this\n",
    "    sigma=1.0 # We can lower the blur to help reduce false detections, again because the bubbles stand our clearly\n",
    ")\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# Step 2: FLANN-based matching\n",
    "index_params = dict(algorithm=1, trees=10)  # KDTree with 10 trees\n",
    "search_params = dict(checks=50)  # Number of checks to perform during the search\n",
    "\n",
    "# Create the FLANN matcher\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Step 3: Match descriptors using FLANN\n",
    "matches = flann.knnMatch(des1, des2, k=2)  # k=2 to apply Lowe's ratio test (this part can be removed)\n",
    "\n",
    "# Step 4: Collect all good matches (no ratio test)\n",
    "good_matches = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "for m in matches:\n",
    "    good_matches.append(m[0])  # Add the best match (m[0]) to good_matches\n",
    "    pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts2.append(kp2[m[0].trainIdx].pt)\n",
    "\n",
    "# Convert the points to numpy arrays\n",
    "pts1 = np.float32(pts1).reshape(-1, 1, 2)\n",
    "pts2 = np.float32(pts2).reshape(-1, 1, 2)\n",
    "\n",
    "# Step 5: Compute the homography using RANSAC\n",
    "H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Step 6: Use the mask to get the inlier matches\n",
    "inliers = [good_matches[i] for i in range(len(good_matches)) if mask[i] == 1]\n",
    "\n",
    "# Step 7: Draw the inlier matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, inliers, None, flags=2)\n",
    "\n",
    "# Step 8: Show the matches\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(img_matches)\n",
    "plt.title('SIFT + FLANN + RANSAC Feature Matching')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49724624",
   "metadata": {},
   "source": [
    "What on earth just happened? FLANN turned our excellent SIFT matches into terrible garbage. This is because SIFT and FLANN are finding a ton of keypoints in the white region of the bubble and then matching anything white, e.g., Spongebob's shirt. Let's change some parameters and see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71db637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different FLANN / RANSAC algorithms\n",
    "# Composite search (algorithm=2) or K-means-based search (algorithm=3)\n",
    "index_params_2 = dict(algorithm=2, trees=5)  # Composite search\n",
    "index_params_3 = dict(algorithm=3, branching=32, iterations=11, checks=64)  # K-means-based search\n",
    "\n",
    "# Using Composite search\n",
    "flann_matcher_2 = cv2.FlannBasedMatcher(index_params_2, {})  \n",
    "matches_2 = flann_matcher_2.match(des1, des2)\n",
    "\n",
    "# Using K-means-based search\n",
    "flann_matcher_3 = cv2.FlannBasedMatcher(index_params_3, {})  \n",
    "matches_3 = flann_matcher_3.match(des1, des2)\n",
    "\n",
    "# Sort matches based on distance (lower distance is better)\n",
    "matches_2 = sorted(matches_2, key=lambda x: x.distance)\n",
    "matches_3 = sorted(matches_3, key=lambda x: x.distance)\n",
    "\n",
    "# Draw the matches for both algorithms (Composite Search and K-means-based Search)\n",
    "img_matches_2 = cv2.drawMatches(img1, kp1, img2, kp2, matches_2[:50], None, flags=2)\n",
    "img_matches_3 = cv2.drawMatches(img1, kp1, img2, kp2, matches_3[:50], None, flags=2)\n",
    "\n",
    "# Display the results using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Displaying the results for Composite search (algorithm=2)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_matches_2)\n",
    "plt.title('FLANN: Composite Search (Algorithm 2)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Displaying the results for K-means-based search (algorithm=3)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_matches_3)\n",
    "plt.title('FLANN: K-means-based Search (Algorithm 3)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in RANSAC\n",
    "# Step 4: Apply RANSAC to filter out outlier matches\n",
    "# Extract the matched keypoints from both images\n",
    "pts1 = np.float32([kp1[m.queryIdx].pt for m in matches_2]).reshape(-1, 1, 2)\n",
    "pts2 = np.float32([kp2[m.trainIdx].pt for m in matches_2]).reshape(-1, 1, 2)\n",
    "\n",
    "# Find the homography using RANSAC\n",
    "# RANSAC will return the inlier matches and the homography matrix\n",
    "H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Step 5: Filter out the inliers using the mask\n",
    "inliers_2 = [m for i, m in enumerate(matches_2) if mask[i] == 1]\n",
    "\n",
    "# Draw the inlier matches for Composite search (algorithm=2)\n",
    "img_matches_2 = cv2.drawMatches(img1, kp1, img2, kp2, inliers_2, None, flags=2)\n",
    "\n",
    "# Repeat the same process for K-means-based search (algorithm=3)\n",
    "pts1_3 = np.float32([kp1[m.queryIdx].pt for m in matches_3]).reshape(-1, 1, 2)\n",
    "pts2_3 = np.float32([kp2[m.trainIdx].pt for m in matches_3]).reshape(-1, 1, 2)\n",
    "\n",
    "# Find the homography using RANSAC for K-means-based matches\n",
    "H_3, mask_3 = cv2.findHomography(pts1_3, pts2_3, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Filter out the inliers\n",
    "inliers_3 = [m for i, m in enumerate(matches_3) if mask_3[i] == 1]\n",
    "\n",
    "# Draw the inlier matches for K-means-based search (algorithm=3)\n",
    "img_matches_3 = cv2.drawMatches(img1, kp1, img2, kp2, inliers_3, None, flags=2)\n",
    "\n",
    "# Step 6: Display the results using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Displaying the results for Composite search (algorithm=2)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_matches_2)\n",
    "plt.title('FLANN: Composite Search with RANSAC (Algorithm 2)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Displaying the results for K-means-based search (algorithm=3)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_matches_3)\n",
    "plt.title('FLANN: K-means-based Search with RANSAC (Algorithm 3)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f48b6",
   "metadata": {},
   "source": [
    "So in this example, RANSAC ends up doing more harm that good --> the types of feature detection and matching we use are highly dependent on application and context. Finally, let's put the keypoints into the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite search (algorithm=2) or K-means-based search (algorithm=3)\n",
    "index_params_2 = dict(algorithm=2, trees=5)  # Composite search\n",
    "index_params_3 = dict(algorithm=3, branching=32, iterations=11, checks=64)  # K-means-based search\n",
    "\n",
    "# Using Composite search\n",
    "flann_matcher_2 = cv2.FlannBasedMatcher(index_params_2, {})  \n",
    "matches_2 = flann_matcher_2.match(des1, des2)\n",
    "\n",
    "# Using K-means-based search\n",
    "flann_matcher_3 = cv2.FlannBasedMatcher(index_params_3, {})  \n",
    "matches_3 = flann_matcher_3.match(des1, des2)\n",
    "\n",
    "# Sort matches based on distance (lower distance is better)\n",
    "matches_2 = sorted(matches_2, key=lambda x: x.distance)\n",
    "matches_3 = sorted(matches_3, key=lambda x: x.distance)\n",
    "\n",
    "# Draw the keypoints on the images (with red 'X')\n",
    "img1_with_kp = cv2.drawKeypoints(img1, kp1, None, color=(255, 0, 0), flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp = cv2.drawKeypoints(img2, kp2, None, color=(255, 0, 0), flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Step 3: Display the results using matplotlib (Separate Plots)\n",
    "# Display Image 1 with keypoints (Larger Image)\n",
    "plt.figure(figsize=(12, 8))  # Larger figure for image 1\n",
    "plt.imshow(img1_with_kp)\n",
    "plt.title('Keypoints on Image 1 with Composite Search')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display Image 2 with keypoints\n",
    "plt.figure(figsize=(8, 6))  # Smaller figure for image 2\n",
    "plt.imshow(img2_with_kp)\n",
    "plt.title('Keypoints on Image 2 with K-means-based Search')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d357d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
